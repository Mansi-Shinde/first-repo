{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM1+SQoGhg3j6GhogtsUTa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mansi-Shinde/first-repo/blob/master/gradient_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2FkGW9-cXWb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "def mean_squared_error(y_true, y_predicted):\n",
        "    cost = np.sum((y_true-y_predicted)**2) / len(y_true) #Calculating the loss or cost\n",
        "    return cost\n",
        " \n",
        "def gradient_descent(x, y, iterations = 1000, learning_rate = 0.0001, stopping_threshold = 1e-6): #Gradient Descent Function\n",
        "    #Initializing weight, bias, learning rate and iterations\n",
        "    current_weight = 0.1\n",
        "    current_bias = 0.01\n",
        "    iterations = iterations\n",
        "    learning_rate = learning_rate\n",
        "    n = float(len(x))\n",
        "    costs = []\n",
        "    weights = []\n",
        "    previous_cost = None\n",
        "    \n",
        "    for i in range(iterations): #Estimation of optimal parameters\n",
        "        y_predicted = (current_weight * x) + current_bias #Making predictions\n",
        "        current_cost = mean_squared_error(y, y_predicted) #Calculationg the current cost\n",
        "        #If the change in cost is less than or equal to stopping_threshold we stop the gradient descent\n",
        "        if previous_cost and abs(previous_cost-current_cost)<=stopping_threshold:\n",
        "            break\n",
        "        previous_cost = current_cost\n",
        "        costs.append(current_cost)\n",
        "        weights.append(current_weight)\n",
        "        #Calculating the gradients\n",
        "        weight_derivative = -(2/n) * sum(x * (y-y_predicted))  \n",
        "        bias_derivative = -(2/n) * sum(y-y_predicted)\n",
        "        #Updating weights and bias\n",
        "        current_weight = current_weight - (learning_rate * weight_derivative)\n",
        "        current_bias = current_bias - (learning_rate * bias_derivative)        \n",
        "        #Printing the parameters for each 1000th iteration\n",
        "        print(f\"Iteration{i+1}:Cost{current_cost},Weight\\{current_weight},Bias{current_bias}\")\n",
        "    #Visualizing the weights and cost at for all iterations\n",
        "    plt.figure(figsize = (8,6))\n",
        "    plt.plot(weights, costs)\n",
        "    plt.scatter(weights, costs, marker='o', color='red')\n",
        "    plt.title(\"Cost vs Weights\")\n",
        "    plt.ylabel(\"Cost\")\n",
        "    plt.xlabel(\"Weight\")\n",
        "    plt.show()\n",
        "    return current_weight, current_bias\n",
        "\n",
        "#Data\n",
        "X = np.array([32.50234527, 53.42680403, 61.53035803, 47.47563963, 59.81320787,\n",
        "           55.14218841, 52.21179669, 39.29956669, 48.10504169, 52.55001444,\n",
        "           45.41973014, 54.35163488, 44.1640495 , 58.16847072, 56.72720806,\n",
        "           48.95588857, 44.68719623, 60.29732685, 45.61864377, 38.81681754])\n",
        "Y = np.array([31.70700585, 68.77759598, 62.5623823 , 71.54663223, 87.23092513,\n",
        "           78.21151827, 79.64197305, 59.17148932, 75.3312423 , 71.30087989,\n",
        "           55.16567715, 82.47884676, 62.00892325, 75.39287043, 81.43619216,\n",
        "           60.72360244, 82.89250373, 97.37989686, 48.84715332, 56.87721319])\n",
        " \n",
        "#Estimating weight and bias using gradient descent\n",
        "estimated_weight, eatimated_bias = gradient_descent(X, Y, iterations=2000)\n",
        "print(f\"Estimated Weight: {estimated_weight}\\nEstimated Bias: {eatimated_bias}\")\n",
        "Y_pred = estimated_weight*X + eatimated_bias #Making predictions using estimated parameters\n",
        "#Plotting the regression line\n",
        "plt.figure(figsize = (8,6))\n",
        "plt.scatter(X, Y, marker='o', color='red')\n",
        "plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='blue',markerfacecolor='red',markersize=10,linestyle='dashed')\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.show()"
      ]
    }
  ]
}